{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"自然语言是由文字构成的，而语言的含义是由单词构成的。即单词是含义的最小单位。因此为了让计算机理解自然语言，首先要让它理解单词含义。\"\n",
    "original_words = jieba.lcut(text)\n",
    "print(original_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定义一组停用词\n",
    "stopwords = {'的','，' ,'。', '是','而','由'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['自然语言', '文字', '构成', '语言', '含义', '单词', '构成', '即', '单词', '含义', '最小', '单位', '因此', '为了', '让', '计算机', '理解', '自然语言', '首先', '要', '让', '它', '理解', '单词', '含义']\n"
     ]
    }
   ],
   "source": [
    "# 过滤不需要的词和标点符合\n",
    "words = []\n",
    "for word in original_words:\n",
    "    if word not in stopwords: words.append(word)\n",
    "print(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['要', '让', '它', '即', '单词', '为了', '单位', '含义', '自然语言', '因此', '计算机', '理解', '首先', '文字', '最小', '构成', '语言']\n"
     ]
    }
   ],
   "source": [
    "# 构建词表\n",
    "id2word = list(set(words))\n",
    "print(id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'要': 0, '让': 1, '它': 2, '即': 3, '单词': 4, '为了': 5, '单位': 6, '含义': 7, '自然语言': 8, '因此': 9, '计算机': 10, '理解': 11, '首先': 12, '文字': 13, '最小': 14, '构成': 15, '语言': 16}\n"
     ]
    }
   ],
   "source": [
    "# 构建字典，记录wordtoid\n",
    "word2id = {}\n",
    "for i, word in enumerate(id2word):\n",
    "    word2id[word] = i\n",
    "print(word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建一个嵌入层\n",
    "embed = nn.Embedding(len(id2word), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0:要       \t[1.9345132  0.10736947 0.29336035 0.9408728  0.20850663]\n",
      " 1:让       \t[-0.50909746 -1.3658268   0.5771052   0.04073388  0.53066367]\n",
      " 2:它       \t[ 0.12392501  0.9760738  -0.61505806  1.5670041  -0.86666095]\n",
      " 3:即       \t[ 0.65840083  1.7487724  -0.9384946   0.18946756 -0.5251572 ]\n",
      " 4:单词      \t[-1.0709012  -1.4285575  -1.1856719  -0.78264064 -2.1043305 ]\n",
      " 5:为了      \t[0.27002236 2.6341054  1.8971457  1.1035075  0.9675678 ]\n",
      " 6:单位      \t[-0.5336467   0.1014543  -1.8363354  -0.5419253   0.00539643]\n",
      " 7:含义      \t[-0.5123825   0.9305645   0.5959784   0.3783801   0.06315218]\n",
      " 8:自然语言    \t[ 1.4717176  -0.47938594 -1.9158726  -1.29773    -0.21339868]\n",
      " 9:因此      \t[-1.6276623  -0.2591723   0.19782028 -1.7223622   0.5523858 ]\n",
      "10:计算机     \t[ 1.0570966   0.42180833 -0.24100237  0.76189935  1.4884875 ]\n",
      "11:理解      \t[-0.57374203 -0.08922265 -1.8212603   1.2246591   0.48289672]\n",
      "12:首先      \t[-1.5239943   0.11415202  1.5546004  -0.49908897 -0.08441312]\n",
      "13:文字      \t[-0.08568203  1.5035596  -0.6089214  -0.01600475 -1.1733674 ]\n",
      "14:最小      \t[ 1.8101832   0.8014079  -1.0209277  -1.3605042  -0.43985495]\n",
      "15:构成      \t[ 0.24428149  0.18027942 -0.15708111 -0.2910551  -2.2123516 ]\n",
      "16:语言      \t[ 0.57745326 -2.019229   -1.1415629   0.04091293  0.4041613 ]\n"
     ]
    }
   ],
   "source": [
    "# 前向传播，传入索引号，得到词向量\n",
    "for k,v in word2id.items():\n",
    "    word_emd = embed(torch.tensor(v))\n",
    "    print(f\"{v:>2}:{k:8}\\t{word_emd.detach().numpy()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
