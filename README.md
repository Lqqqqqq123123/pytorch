说明

该仓库为深度学习代码学习，希望能在2个月内入门深度学习代码！！！

在保研期间过了很多理论知识，所以现在很多基础知识都很清晰，学习起来非常舒服！

## 每日的简单记录

### 第一天 2025.9.27

正式开始学习，看到尚硅谷出了深度学习课程，发现确实不错。

今天才知道不用 **pytorch**，神经网络到底怎么写，非常有意思！当然，今天的学习提醒我必须恶补python基础了，计划把之前学过的算法全部都用python实现一遍。


### 第二天 2025.9.28 

学了一些pandas以及numpy的知识，也用numpy实现了最简单的神经网络的批处理（也就是把训练集 n * xxx）划分为（batch_size * xxx），具体就是用个for循环，当然，为了提升python代码能力，已经开始把c++的板子用python实现了，但是来这个深度学习仓库记录有点违和...

**比较重要的知识：**
正常list切片：[start : end : step]；numpy这种方式，还支持布尔切片（可以是布尔数组，也可以是布尔条件）,花式索引（其实就是很多索引对）
而pandas得数据结构分为series以及dataframe，其中series是一维带标签数组，默认为从0开始的数字索引；dataframe是一个二维表格数据，也就是既有行索引，也有列索引。

### 第三天 2025.9.29
注：今天敲了8道模板题，python代码能力感觉得到了很大的提升，再接再厉！
今天主要学习了深度学习用到的数学知识
1）导数的定义以及在工程应用中如何求导数（数值微分）为了提高精度，可以用中心差分这一技巧
2）偏导数实际就是多元函数的导数，只不过在计算某个变量的偏导数时，把其他变量看成常数。不同变量的偏导组成的向量就是梯度，它指明了函数变化最快的方向，其模长就是变化率。

### 第四天 2025.10.02
前两天出去玩去了，hhh。
今天主要学了梯度下降算法Sgd，但是直接用中心差分计算求解速度很慢，而优化算法BP能够加快梯度的计算。
BP引出了计算图的概念，其本质就是链式求导法则，当我们从损失函数出发，来求解不同参数对于损失函数的导数时，就可以利用之前求过的梯度来计算。

![](./imgs/1.png)
### 第五天 2025.10.03
##### 1. 今天主要复习了各种优化算法，比如：

1) **Sgd:**  在深度学习中 Sgd 就是 Minibatch Sgd
2) **Momentum:**  引入一个动量项，使得跨过鞍点，同时加块学习速度，缺点是有点激进，容易震荡
3) **AdaGrad:**  对于不同参数，有不同的学习率，其实就是给学习率乘了一个对应的根号下历史梯度平方和的倒数。

$$
h \leftarrow h + \nabla^2
\\
W \leftarrow W - \eta \frac{1}{\sqrt{h}} \nabla
$$

4. **RMSProp：** 对AdaGrad的优化，AdaGrad一直在累计过去的梯度，导致学到后面学不动了，也就是学习率很小了。为了应对这个问题，RMSProp在考虑过去的梯度平方和时，会逐步的削减它，也就是给$h$乘了一个小于1的系数$a$。具体如下：

$$
h \leftarrow ah + (1-a)\nabla^2
\\
W \leftarrow W - \eta \frac{1}{\sqrt{h}} \nabla
$$

5. Adam：融合了 **Momentum** 和 **AdaGrad** 的优点，结合了一阶矩（动量）和二阶矩（梯度平方的指数平均），实现了快速收敛与良好的稳定性。

$$
v \leftarrow \alpha_1 v + (1 - \alpha_1) \nabla
\\
h \leftarrow \alpha_2 h + (1 - \alpha_2) \nabla^2
\\
\hat{v} = \frac{v}{1 - \alpha_1^t}
\\
\hat{h} = \frac{h}{1 - \alpha_2^t}
\\
W \leftarrow W - \eta \frac{\hat{v}}{\sqrt{\hat{h}} + \epsilon}
$$



##### 2. 同时学习了一些参数的初始化方式

​	参数初始化方案的选择在神经网络学习中起着举足轻重的作用，它对保持数值稳定性至关重要。此外，这些初始化方案的选择可以与激活函数的选择有趣的结合在一起。我们选择哪个激活函数以及如何初始化参数，可以决定优化算法收敛的速度有多快；糟糕选择可能会导致我们在训练时遇到梯度爆炸或梯度消失。

##### 3. 数据的正则化



### 第六天 2025.10.04

​	今天正式开始了pytorch的学习，主要就是tensor的创建、运算、一些统计方法（max,min,unique,sort,sum,std...)



